{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic - Machine Learning from Disaster "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Daniel Betzalel\n",
    "- https://www.kaggle.com/danielbetzalel\n",
    "- Shai Odeni\n",
    "- https://www.kaggle.com/shaiodeni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL;DR\n",
    "The assignment was to predict the survivors on the Titanic ship according to the given features. We separated the work into 6 parts:\n",
    "\n",
    "1. **Imports and Definitions** - Importing necessary libraries and setting up global settings.\n",
    "2. **Data Investigation EDA** - Exploring and understanding the dataset using statistical analysis and visualization.\n",
    "3. **Data Cleaning and Preprocessing** - Handling missing values, outliers, and encoding categorical variables.\n",
    "4. **Feature Selection** - Choosing the most relevant features for the model.\n",
    "5. **Model Selection and Training** - Choosing, training, and tuning a machine learning model.\n",
    "6. **Tests Model** - Evaluating the model's performance on a test set.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Imports and Definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy, matplotlib, etc.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# sklearn imports\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn import pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn import neural_network\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeavePOut\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import os\n",
    "\n",
    "\n",
    "# define plt settings\n",
    "sns.set_theme()\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "plt.rcParams[\"axes.labelsize\"] = 20\n",
    "plt.rcParams[\"xtick.labelsize\"] = 20\n",
    "plt.rcParams[\"ytick.labelsize\"] = 20\n",
    "plt.rcParams[\"legend.fontsize\"] = 20\n",
    "plt.rcParams[\"legend.markerscale\"] = 1.5\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "plt.rcParams[\"legend.title_fontsize\"] = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_GRAPHS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define the input and output folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"input/\"\n",
    "\n",
    "train_data_path = os.path.join(input_folder, \"train.csv\")\n",
    "test_data_path = os.path.join(input_folder, \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the traning data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the csv data to variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_data_path)\n",
    "\n",
    "test_data = pd.read_csv(test_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Data Investigation EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns that we will drop are:\n",
    "- PassengerId: it is just an index\n",
    "- Name: it is not relevant for the model\n",
    "- Ticket: it is not relevant for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "DROP_DATA = [\"PassengerId\", \"Name\", \"Ticket\"]\n",
    "def remove_Unused_Columns(data):\n",
    "    data = data.drop(DROP_DATA, axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "def convet_gender_to_numric(df):\n",
    "    \"\"\"   \n",
    "    \"sex\" column ===> male = 1, female = 0\n",
    "    \"\"\"\n",
    "    df['Sex'] = le.fit_transform(df['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = remove_Unused_Columns(train_data)\n",
    "\n",
    "# passenger_ids save the passenger ids for the test data for the submission\n",
    "passenger_ids = test_data['PassengerId']\n",
    "test_data = remove_Unused_Columns(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert Men/Women to 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convet_gender_to_numric(train_data)\n",
    "\n",
    "convet_gender_to_numric(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get summary statistics for the training dataset show only the numerical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of missing values in the training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains missing values in the following columns:\n",
    "\n",
    "1) Age: 177 missing values\n",
    "2) Cabin: 687 missing values (cabin has a lot of missing values)\n",
    "3) Embarked: 2 missing values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the data types of the columns in the training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that most of the data is int64 or float64, only the Cabin and Embarked Are object types (String)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_calculate(df, column):\n",
    "    # Plot the survival rate\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x=column, hue='Survived', data=df)\n",
    "    plt.title(f'Survival Rate by {column}')\n",
    "    plt.show()\n",
    "\n",
    "    # Group by column and 'Survived', then get the size of each group\n",
    "    grouped = df.groupby([column, 'Survived']).size()\n",
    "\n",
    "    # Calculate the percentage of survivors\n",
    "    percentage_survived = grouped.xs(1, level='Survived') / grouped.groupby(level=column).sum() * 100\n",
    "\n",
    "    # Print the percentage of survivors\n",
    "    print(f\"Percentage of survivors for each {column}:\")\n",
    "    print(percentage_survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we will explore the relationship between the survival rate and some of the features in the dataset.\n",
    "- After each graph, we will print the percentage of survivors for each category in the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_GRAPHS:\n",
    "    plot_and_calculate(train_data, 'Embarked')\n",
    "    plot_and_calculate(train_data, 'Parch')\n",
    "    plot_and_calculate(train_data, 'SibSp')\n",
    "    plot_and_calculate(train_data, 'Sex')\n",
    "    plot_and_calculate(train_data, 'Pclass')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can study from this graph\n",
    "1) From port C more people survived but from S and Q most of the people died\n",
    "2) Most women survived (74%)\n",
    "3) Most people from Pclass 1 survived (63%) but most people from Pclass 3 died (76% died)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pairplot Visualizing Correlation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we will display the pairplot of the data we can see the correlation between the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_GRAPHS:\n",
    "    sns.pairplot(train_data[['Survived', 'Pclass', 'Age', 'Fare', 'Sex']], hue='Survived')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heatmap for correlation matrix**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_GRAPHS:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    corr_matrix = train_data.corr(numeric_only=True)\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the correlation between the Survived and features like Pclass, Fare are high.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values in the train dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill Missing Values\n",
    "- Categorical columns with the most frequent value \n",
    "- Numerical columns with the mean\n",
    "- Drop the Cabin column due to too many missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# fill missing values, the \n",
    "\n",
    "\n",
    "def fill_missing_values(data):\n",
    "    data['Age'] = data['Age'].fillna(data['Age'].mean())\n",
    "    data['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])\n",
    "    data = handle_missing_values(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def drop_missing_values(data):\n",
    "    data.drop(columns=['Cabin'], inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def handle_missing_values(data):    \n",
    "    # Impute numerical columns with mean\n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    data[data.select_dtypes(include=['number']).columns] = imputer_num.fit_transform(data.select_dtypes(include=['number']))\n",
    "    \n",
    "    # Impute categorical columns with constant value 'missing'\n",
    "    imputer_cat = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "    data[data.select_dtypes(exclude=['number']).columns] = imputer_cat.fit_transform(data.select_dtypes(exclude=['number']))\n",
    "    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there's only two people with NaN, after searching about them it was find out they board in Southampton \n",
    "train_data[\"Embarked\"] = train_data.loc[:, 'Embarked'].fillna('S')\n",
    "\n",
    "train_data = drop_missing_values(train_data)\n",
    "train_data = fill_missing_values(train_data)\n",
    "\n",
    "# test data\n",
    "test_data = drop_missing_values(test_data)\n",
    "test_data = fill_missing_values(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.isnull().sum())\n",
    "\n",
    "print(test_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Categorical Variables into Numerical Values\n",
    "- Embarked use One-hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical(data):\n",
    "    # One-hot encode the categorical columns\n",
    "    data = pd.get_dummies(data, columns=['Embarked'], drop_first=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = encode_categorical(train_data)\n",
    "\n",
    "# test_data = encode_categorical(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize Numerical Features\n",
    "- Standardize Age and Fare to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_data(data):\n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "    data[['Age', 'Fare']] = scaler.fit_transform(data[['Age', 'Fare']])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = scale_data(train_data)\n",
    "\n",
    "# test_data = scale_data(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Features\n",
    " - Family Size from SibSp and Parch\n",
    " - Is Alone from the FamilySize\n",
    " - Remove SibSp and Parch columns becuse of the new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(data):\n",
    "    \"\"\"\n",
    "    Create new features for the dataset.\n",
    "    \"\"\"\n",
    "    # Create FamilySize feature \n",
    "    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n",
    "\n",
    "    # Create IsAlone feature\n",
    "    data['IsAlone'] = (data['FamilySize'] == 1).astype(int)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def drop_features(data):\n",
    "    data.drop(columns=['SibSp', 'Parch'], inplace=True)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_data = create_features(train_data)\n",
    "train_data = drop_features(train_data)\n",
    "# test data\n",
    "test_data = create_features(test_data)\n",
    "test_data = drop_features(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.head())\n",
    "\n",
    "print(\"\\n\\nmissing values in train data:\\n\")\n",
    "print(train_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_GRAPHS:\n",
    "    corr_matrix = train_data.corr(numeric_only=True)\n",
    "    sorted_columns = corr_matrix.abs().sort_values('Survived', ascending=False).index\n",
    "    sorted_corr_matrix = corr_matrix.reindex(index=sorted_columns, columns=sorted_columns)\n",
    "\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=sorted_corr_matrix.values,\n",
    "        x=sorted_corr_matrix.columns,\n",
    "        y=sorted_corr_matrix.columns,\n",
    "        colorscale='Viridis',\n",
    "        text=sorted_corr_matrix.values.round(2),\n",
    "        texttemplate=\"%{text}\",\n",
    "        showscale=True\n",
    "    ))\n",
    "    fig.update_layout(title='Correlation Matrix Sorted by \"Survived\"', width=1000, height=800)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL;DR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the data into features and target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(columns='Survived')\n",
    "t = train_data['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X shape: \", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# use column transformer to insert different transformers for each column\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numerical_cols = X.select_dtypes(include=['int32', 'int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'bool']).columns\n",
    "all_cols = list(categorical_cols) + list(numerical_cols)\n",
    "ct_enc_std = ColumnTransformer([\n",
    "            (\"encoding\", OrdinalEncoder(), categorical_cols),\n",
    "            (\"standard\", StandardScaler(), numerical_cols)])\n",
    "X_enc = pd.DataFrame(ct_enc_std.fit_transform(X, t), columns=all_cols)\n",
    "test_X_enc = pd.DataFrame(ct_enc_std.fit_transform(test_data), columns=X_enc.columns.values.tolist())\n",
    "X = X_enc\n",
    "test_data = test_X_enc\n",
    "print(\"X shape: \", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPI functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate key performance indicators (KPIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpis(cm):\n",
    "    \"\"\"\n",
    "    Calculate key performance indicators (KPIs) from a confusion matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    cm (numpy.ndarray): Confusion matrix\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing precision, recall, specificity, false positive rate, and accuracy.\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    fpr = fp / (fp + tn)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return {\n",
    "        'Precision': round(precision, 2),\n",
    "        'Recall': round(recall, 2),\n",
    "        'Specificity': round(specificity, 2),\n",
    "        'FPR': round(fpr, 2),\n",
    "        'Accuracy': round(accuracy, 2),\n",
    "        'F1 Score': round(f1, 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Show the KPI table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "def plot_kpi_table(kpi_values):\n",
    "    \"\"\"\n",
    "    Display KPIs in a table using Plotly.\n",
    "    \n",
    "    Parameters:\n",
    "    kpi_values (dict): Dictionary containing KPI names as keys and KPI values as values.\n",
    "    \"\"\"\n",
    "    fig = go.Figure(data=[go.Table(\n",
    "        header=dict(values=[\"KPI\", \"Value\"],\n",
    "                    fill_color='paleturquoise',\n",
    "                    align='left'),\n",
    "        cells=dict(values=[list(kpi_values.keys()), list(kpi_values.values())],\n",
    "                   fill_color='lavender',\n",
    "                   align='left'))\n",
    "    ])\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Key Performance Indicators (KPIs)\",\n",
    "        autosize=True,\n",
    "        width=500,\n",
    "        height=400\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Display the confusion matrix as a heatmap and print key performance indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_KPIs(cm):\n",
    "    \"\"\"\n",
    "    Display the confusion matrix as a heatmap using Plotly and print key performance indicators.\n",
    "    \n",
    "    Parameters:\n",
    "    cm (numpy.ndarray): Confusion matrix\n",
    "    \"\"\"\n",
    "    # Create a DataFrame for the confusion matrix with proper labels\n",
    "    cm_df = pd.DataFrame(cm, index=['actual_0', 'actual_1'], columns=['predicted_0', 'predicted_1'])\n",
    "    \n",
    "    # Plot the confusion matrix as a heatmap using Plotly\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=cm_df.values,\n",
    "        x=cm_df.columns,\n",
    "        y=cm_df.index,\n",
    "        colorscale='Blues',  \n",
    "        text=cm_df.values,\n",
    "        texttemplate=\"%{text}\",\n",
    "        hoverinfo=\"text\"\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Confusion Matrix\",\n",
    "        xaxis_title=\"Predicted Label\",\n",
    "        yaxis_title=\"Actual Label\",\n",
    "        font=dict(size=18)\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Calculate KPIs and show table\n",
    "    kpi_values = kpis(cm)\n",
    "    plot_kpi_table(kpi_values)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- calculate score and loss from cv (KFold or LPO) and display graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import RepeatedKFold, LeavePOut\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn import metrics\n",
    "\n",
    "def get_cv_score_and_loss(X, t, model, k=None, p=None, show_score_loss_graphs=True, show_cm=True):\n",
    "    # Initialize a DataFrame to store scores and losses\n",
    "    scores_losses_df = pd.DataFrame(columns=['fold_id', 'split', 'score', 'loss'])\n",
    "\n",
    "    # Determine the type of cross-validation to use\n",
    "    if k is not None:\n",
    "        cv = RepeatedKFold(n_splits=k, n_repeats=10, random_state=42)  # K-Fold cross-validation\n",
    "    elif p is not None:\n",
    "        cv = LeavePOut(p)  # Leave-P-Out cross-validation\n",
    "    else:\n",
    "        raise ValueError('You need to specify either k or p for cross-validation.')\n",
    "\n",
    "    # List to store confusion matrices for each fold\n",
    "    conf_matrix_list_of_arrays = []\n",
    "    \n",
    "    # Iterate over each fold in the cross-validation\n",
    "    for i, (train_ids, val_ids) in enumerate(cv.split(X)):\n",
    "        # Split the data into training and validation sets\n",
    "        X_train = X.loc[X.index.intersection(train_ids)]\n",
    "        t_train = t.loc[t.index.intersection(train_ids)]\n",
    "        X_val = X.loc[X.index.intersection(val_ids)]\n",
    "        t_val = t.loc[t.index.intersection(val_ids)]\n",
    "\n",
    "        # Fit the model on the training set\n",
    "        model.fit(X_train, t_train)\n",
    "\n",
    "        # Predict on the training and validation sets\n",
    "        y_train = abs(model.predict(X_train))\n",
    "        y_val = abs(model.predict(X_val))\n",
    "        \n",
    "        # Store scores and losses\n",
    "        scores_losses_df.loc[len(scores_losses_df)] = [i, 'train', model.score(X_train, t_train), metrics.log_loss(y_train, t_train)]\n",
    "        scores_losses_df.loc[len(scores_losses_df)] = [i, 'val', model.score(X_val, t_val), metrics.log_loss(y_val, t_val)]\n",
    "\n",
    "        # Compute the confusion matrix for the validation set and store it\n",
    "        conf_matrix = confusion_matrix(t_val, y_val)\n",
    "        conf_matrix_list_of_arrays.append(conf_matrix)\n",
    "\n",
    "    # Separate the scores and losses for training and validation sets\n",
    "    val_scores_losses_df = scores_losses_df[scores_losses_df['split'] == 'val']\n",
    "    train_scores_losses_df = scores_losses_df[scores_losses_df['split'] == 'train']\n",
    "\n",
    "    # Calculate mean scores and losses\n",
    "    mean_val_score = val_scores_losses_df['score'].mean()\n",
    "    mean_val_loss = val_scores_losses_df['loss'].mean()\n",
    "    mean_train_score = train_scores_losses_df['score'].mean()\n",
    "    mean_train_loss = train_scores_losses_df['loss'].mean()\n",
    "\n",
    "    # Calculate the mean confusion matrix across all folds\n",
    "    mean_of_conf_matrix_arrays = np.mean(conf_matrix_list_of_arrays, axis=0)\n",
    "\n",
    "    # Plot the score and loss graphs if requested\n",
    "    if show_score_loss_graphs:\n",
    "        fig = px.line(scores_losses_df, x='fold_id', y='score', color='split', title=f'Mean Val Score: {mean_val_score:.2f}, Mean Train Score: {mean_train_score:.2f}')\n",
    "        fig.show()\n",
    "        fig = px.line(scores_losses_df, x='fold_id', y='loss', color='split', title=f'Mean Val Loss (CE): {mean_val_loss:.2f}, Mean Train Loss (CE): {mean_train_loss:.2f}')\n",
    "        fig.show()\n",
    "        \n",
    "\n",
    "        # Create a DataFrame to display the scores and losses as a table\n",
    "        scores_losses_table = pd.DataFrame({\n",
    "            'Metric': ['Mean CV Val Score', 'Mean CV Val Loss (CE)', 'Mean CV Train Score', 'Mean CV Train Loss (CE)'],\n",
    "            'Value': [f\"{mean_val_score:.2f}\", f\"{mean_val_loss:.2f}\", f\"{mean_train_score:.2f}\", f\"{mean_train_loss:.2f}\"]\n",
    "        })\n",
    "\n",
    "        # Display the table\n",
    "        print(scores_losses_table.to_string(index=False))\n",
    "        \n",
    "        \n",
    "\n",
    "    # Display the confusion matrix KPIs if requested\n",
    "    if show_cm:\n",
    "        confusion_matrix_KPIs(mean_of_conf_matrix_arrays)\n",
    "        \n",
    "\n",
    "    return mean_val_score, mean_val_loss, mean_train_score, mean_train_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we will search for the strongest features that affect the survival rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def feature_selec(X, y, n):\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'bool']).columns\n",
    "    all_cols = categorical_cols.tolist() + numerical_cols.tolist()\n",
    "    ct_enc_std = ColumnTransformer([\n",
    "                (\"encoding\", OrdinalEncoder(), categorical_cols),\n",
    "                (\"standard\", StandardScaler(), numerical_cols)])\n",
    "    X_encoded = pd.DataFrame(ct_enc_std.fit_transform(X, y), columns=all_cols)\n",
    "\n",
    "    selector = RFE(SGDRegressor(random_state=42), n_features_to_select=n).\\\n",
    "    fit(X_encoded, y)\n",
    "\n",
    "    X_encoded.loc[:, selector.support_]\n",
    "\n",
    "    # print the fetures selection list\n",
    "    features = X_encoded.loc[:, selector.support_].columns.tolist()\n",
    "    print(\"features: \", features)\n",
    "\n",
    "    # keep only the feature selection list\n",
    "    X = X[features]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def perform_feature_selection(X, y, n_features_to_select=None):\n",
    "    \"\"\"\n",
    "    Perform feature selection using Recursive Feature Elimination (RFE) and\n",
    "    Recursive Feature Elimination with Cross-Validation (RFECV).\n",
    "    \n",
    "    Parameters:\n",
    "    X (pd.DataFrame): Feature set\n",
    "    y (pd.Series): Target variable\n",
    "    n_features_to_select (int): Number of features to select (for RFE). If None, RFECV will be used.\n",
    "    \n",
    "    Returns:\n",
    "    selected_features (list): List of selected feature names\n",
    "    importance (pd.Series): Feature importance scores\n",
    "    \"\"\"\n",
    "    # Initialize the base estimator (Logistic Regression in this case)\n",
    "    estimator = LogisticRegression(random_state=42)\n",
    "    \n",
    "    if n_features_to_select is not None:\n",
    "        # Perform RFE\n",
    "        selector = RFE(estimator, n_features_to_select=n_features_to_select, step=1)\n",
    "    else:\n",
    "        # Perform RFECV\n",
    "        selector = RFECV(estimator, step=1, cv=StratifiedKFold(5), scoring='accuracy', min_features_to_select=1)\n",
    "    \n",
    "    # Fit the selector\n",
    "    selector = selector.fit(X, y)\n",
    "    \n",
    "    # Get selected features\n",
    "    selected_features = X.columns[selector.support_].tolist()\n",
    "    \n",
    "    # Get feature importance\n",
    "    if hasattr(selector.estimator_, 'coef_'):\n",
    "        importances = selector.estimator_.coef_[0]\n",
    "    elif hasattr(selector.estimator_, 'feature_importances_'):\n",
    "        importances = selector.estimator_.feature_importances_\n",
    "    else:\n",
    "        print(\"Warning: Could not extract feature importances.\")\n",
    "        importances = np.zeros(len(X.columns))\n",
    "    \n",
    "    # Ensure importances match the number of selected features\n",
    "    if len(importances) != len(selected_features):\n",
    "        print(f\"Warning: Number of importance scores ({len(importances)}) \"\n",
    "              f\"does not match number of selected features ({len(selected_features)}).\")\n",
    "        print(\"Using absolute ranks as importance scores.\")\n",
    "        importances = np.abs(selector.ranking_)\n",
    "        importances = max(importances) - importances  # Invert so that most important feature has highest score\n",
    "    \n",
    "    # Create importance Series\n",
    "    importance = pd.Series(importances, index=selected_features)\n",
    "    importance = importance.sort_values(ascending=False)\n",
    "    \n",
    "    # Plot results\n",
    "    if n_features_to_select is None:\n",
    "        # Plot number of features vs. cross-validation scores for RFECV\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, len(selector.cv_results_['mean_test_score']) + 1), \n",
    "                 selector.cv_results_['mean_test_score'])\n",
    "        plt.xlabel(\"Number of features selected\")\n",
    "        plt.ylabel(\"Cross validation score (accuracy)\")\n",
    "        plt.title(\"RFECV: Optimal number of features\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    importance.plot(kind='bar')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return selected_features, importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the features\n",
    "print(\"All features:\", X.columns)\n",
    "\n",
    "# Perform feature selection\n",
    "best_features, feature_importance = perform_feature_selection(X, t)\n",
    "\n",
    "print(\"Selected features:\", best_features)\n",
    "print(\"\\nFeature importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Update X with selected features\n",
    "X_selected_features = X[best_features]\n",
    "\n",
    "\n",
    "# Optional select features by amount\n",
    "# n_features_to_select = 5\n",
    "# best_features = feature_selec(X, t, n_features_to_select)\n",
    "# X_selected_features = X[selected_features]\n",
    "\n",
    "\n",
    "\n",
    "print(\"X Shape: \", X.shape)\n",
    "print(\"X Selected Shape: \", X_selected_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected_features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see from the graph that the optimal number of features is 6\n",
    "- features selected: ['Embarked', 'Pclass', 'Sex', 'Age', 'FamilySize', 'IsAlone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will compare 2 search methods, GridSearchCV and RandomizedSearchCV to find the best hyperparameters for the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from scipy.stats import uniform\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compare_search_methods(X, t, cv_splits=5, cv_repeats=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Function to compare GridSearchCV and RandomizedSearchCV for parameter optimization of SGDClassifier.\n",
    "    Performs feature selection using RFECV and plots the comparison graph.\n",
    "    \n",
    "    Parameters:\n",
    "    X (pd.DataFrame): Feature set.\n",
    "    t (pd.Series): Target variable.\n",
    "    cv_splits (int): Number of splits for cross-validation.\n",
    "    cv_repeats (int): Number of repeats for cross-validation.\n",
    "    random_state (int): Random state for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing the best hyperparameters and scores for GridSearchCV and RandomizedSearchCV.\n",
    "    \"\"\"\n",
    "    # Feature selection using RFECV\n",
    "    selector = RFECV(estimator=SGDClassifier(loss='log_loss', random_state=random_state),\n",
    "                     cv=RepeatedKFold(n_splits=cv_splits, n_repeats=cv_repeats, random_state=random_state))\n",
    "    X_selected_features = pd.DataFrame(selector.fit_transform(X, t), columns=X.columns[selector.support_])\n",
    "    \n",
    "    # GridSearchCV\n",
    "    hyper_parameters_grid = {'penalty': ('l2', 'l1', 'elasticnet'), 'alpha': [0.0001, 0.001, 0.01, 0.1]}\n",
    "    gs_model = GridSearchCV(SGDClassifier(random_state=random_state), hyper_parameters_grid, cv=5)\n",
    "    gs_model.fit(X_selected_features, t)\n",
    "    \n",
    "    # RandomizedSearchCV\n",
    "    distributions = {'alpha': uniform(loc=0, scale=1), 'penalty': ['l2', 'l1', 'elasticnet']}\n",
    "    rs_model = RandomizedSearchCV(SGDClassifier(random_state=random_state), distributions, cv=5, random_state=random_state)\n",
    "    rs_model.fit(X_selected_features, t)\n",
    "    \n",
    "    # Print results\n",
    "    print('GridSearchCV results:')\n",
    "    print('Accuracy score:', gs_model.best_score_)\n",
    "    print('Best parameters:', gs_model.best_params_)\n",
    "    print()\n",
    "    \n",
    "    print('RandomizedSearchCV results:')\n",
    "    print('Accuracy score:', rs_model.best_score_)\n",
    "    print('Best parameters:', rs_model.best_params_)\n",
    "    print()\n",
    "    \n",
    "    # Plotting the comparison graph\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(name='GridSearchCV', x=['Accuracy'], y=[gs_model.best_score_], marker_color='blue'))\n",
    "    fig.add_trace(go.Bar(name='RandomizedSearchCV', x=['Accuracy'], y=[rs_model.best_score_], marker_color='orange'))\n",
    "    fig.update_layout(title='Comparison of GridSearchCV and RandomizedSearchCV',\n",
    "                      xaxis_title='Search Method',\n",
    "                      yaxis_title='Accuracy Score',\n",
    "                      barmode='group')\n",
    "    fig.show()\n",
    "    \n",
    "    # Return the best hyperparameters and scores\n",
    "    best_params = {\n",
    "        'GridSearchCV': {\n",
    "            'best_score': gs_model.best_score_,\n",
    "            'best_params': gs_model.best_params_\n",
    "        },\n",
    "        'RandomizedSearchCV': {\n",
    "            'best_score': rs_model.best_score_,\n",
    "            'best_params': rs_model.best_params_\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return gs_model, rs_model, best_params\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Search VS Random Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare search methods and get best parameters\n",
    "gs_model, rs_model, best_params = compare_search_methods(X, t)\n",
    "print(f\"Best parameters found: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can see that the difference is not significant, and the grid search is more expensive than the random search.<br> Despite this, we will stick with the grid search for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ensembles**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_fold = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the BaggingClassifier with SGDClassifier as the base estimator\n",
    "bag_fold_model = BaggingClassifier(\n",
    "    estimator=SGDClassifier(penalty='elasticnet', alpha=0.001),\n",
    "    n_estimators=20,\n",
    "    random_state=42,\n",
    "    bootstrap=False\n",
    ").fit(X_selected_features, t)\n",
    "\n",
    "# Display the accuracy score of the model on the training data\n",
    "accuracy_score = bag_fold_model.score(X_selected_features, t)\n",
    "print('Accuracy score for classification with Bagging and SGDClassifier:', accuracy_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_score, val_loss, train_score, train_loss = get_cv_score_and_loss(X_selected_features, t, bag_fold_model, k=num_of_fold, show_score_loss_graphs=True, show_cm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bootstrap Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the BaggingClassifier with SGDClassifier as the base estimator\n",
    "# and enable bootstrap sampling\n",
    "bag_fold_model = BaggingClassifier(\n",
    "    estimator=SGDClassifier(penalty='elasticnet', alpha=0.001),\n",
    "    n_estimators=20,\n",
    "    random_state=42,\n",
    "    bootstrap=True  # Enable bootstrap sampling\n",
    ").fit(X_selected_features, t)\n",
    "\n",
    "# Display the accuracy score of the model on the training data\n",
    "accuracy_score = bag_fold_model.score(X_selected_features, t)\n",
    "print('Accuracy score for classification with Bootstrap Bagging and SGDClassifier:', accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_score, val_loss, train_score, train_loss = get_cv_score_and_loss(X_selected_features, t, bag_fold_model, k=num_of_fold, show_score_loss_graphs=True, show_cm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get score with ada boosting\n",
    "ada_boost_model = AdaBoostClassifier(n_estimators=100, random_state=42).fit(X_selected_features, t)\n",
    "print('Accuracy score for classification:')\n",
    "print('ada_boost_model', ada_boost_model.score(X_selected_features, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_score, val_loss, train_score, train_loss = get_cv_score_and_loss(X_selected_features, t, ada_boost_model, k=num_of_fold, p=None, show_score_loss_graphs=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the Boosting model has a higher accuracy score than the Bagging model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN vs LDA vs NBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **k-Nearest Neighbors (KNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RepeatedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define hyperparameters\n",
    "hyper_parameters = {'n_neighbors': list(range(1, 100))}\n",
    "\n",
    "# KNN model with GridSearchCV\n",
    "gs_neigh_model = GridSearchCV(\n",
    "    KNeighborsClassifier(), \n",
    "    hyper_parameters, \n",
    "    cv=RepeatedKFold(n_splits=10, n_repeats=10, random_state=42)\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV on your selected features X_selected_features and target t\n",
    "gs_neigh_model.fit(X_selected_features, t)\n",
    "\n",
    "# Print accuracy score and best parameters\n",
    "print('Accuracy score for classification:')\n",
    "print('gs_neigh_model', gs_neigh_model.best_score_)\n",
    "print('best params', gs_neigh_model.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_score, val_loss, train_score, train_loss = get_cv_score_and_loss(X_selected_features, t, gs_neigh_model, k=num_of_fold, p=None, show_score_loss_graphs=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Naive Bayes (NBC)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "# Create a pipeline that first transforms the data to a non-negative range and then fits the model\n",
    "NBC_model_pipeline = make_pipeline(\n",
    "    MinMaxScaler(),  # Scales data to range [0, 1]\n",
    "    MultinomialNB()\n",
    ")\n",
    "\n",
    "print('Accuracy score for classification:')\n",
    "NBC_model_pipeline.fit(X_selected_features, t)\n",
    "print('nbc_model', NBC_model_pipeline.score(X_selected_features, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_score, val_loss, train_score, train_loss = get_cv_score_and_loss(X_selected_features, t, NBC_model_pipeline, k=num_of_fold, p=None, show_score_loss_graphs=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Linear Discriminant Analysis (LDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# Show score of LDA on the data\n",
    "print('Accuracy score for classification:')\n",
    "lda_model = LinearDiscriminantAnalysis().fit(X_selected_features, t)\n",
    "print('lda_model', lda_model.score(X_selected_features, t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_score, val_loss, train_score, train_loss = get_cv_score_and_loss(X_selected_features, t, lda_model, k=num_of_fold, p=None, show_score_loss_graphs=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  We can see that the best model is KNN, so we will use it to predict the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**<br>\n",
    "The training and validation sets in the Titanic dataset are similar in how their data is spread out, the missing information, and the changes made to the data,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gs_neigh_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keep only the feature selection list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_selected_features = test_data[best_features]\n",
    "\n",
    "print(\"test_data_selected_features: \", test_data_selected_features.shape)\n",
    "test_data_selected_features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(test_data_selected_features).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create sumbission file for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({'PassengerId': passenger_ids, 'Survived': predictions})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "time = pd.Timestamp.now()\n",
    "formatted_time = time.strftime('%H:%M:%S')\n",
    "print(f\"Your submission was successfully saved at time {formatted_time}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL;DR\n",
    "We've tried 3 models (SGD, MLP, Logistic Regression). <br>The highest score was from the MLP model.\n",
    "After the model selection, we made hyperparameters and chose the best of them. We also tried different learning rates.<br> We found out that 0.01 is the best one.\n",
    "<br>We've dropped some of the features (name, ticket, cabin) and made some others to replace them in a more accurate way, such as familySize & isAlone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## References:\n",
    "Encyclopedia of the Titanic and its passengers:<br>\n",
    "https://www.encyclopedia-titanica.org/\n",
    "\n",
    "YouTube:<br>\n",
    "To explain the structure of the ship, and the impact of the iceberg."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titanic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

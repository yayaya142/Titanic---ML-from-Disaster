{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic - Machine Learning from Disaster "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Daniel Betzalel\n",
    "- https://www.kaggle.com/danielbetzalel\n",
    "- Shai Odeni\n",
    "- https://www.kaggle.com/shaiodeni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL;DR\n",
    "The assignment was to predict the survivors on the Titanic ship according to the given features. We separated the work into 6 parts:\n",
    "\n",
    "1. **Imports and Definitions** - Importing necessary libraries and setting up global settings.\n",
    "2. **Data Investigation EDA** - Exploring and understanding the dataset using statistical analysis and visualization.\n",
    "3. **Data Cleaning and Preprocessing** - Handling missing values, outliers, and encoding categorical variables.\n",
    "4. **Feature Selection** - Choosing the most relevant features for the model.\n",
    "5. **Model Selection and Training** - Choosing, training, and tuning a machine learning model.\n",
    "6. **Tests Model** - Evaluating the model's performance on a test set.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Imports and Definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy, matplotlib, etc.\n",
    "# import numpy, matplotlib, etc.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# sklearn imports\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn import pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn import neural_network\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeavePOut\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# define plt settings\n",
    "sns.set_theme()\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "plt.rcParams[\"axes.labelsize\"] = 20\n",
    "plt.rcParams[\"xtick.labelsize\"] = 20\n",
    "plt.rcParams[\"ytick.labelsize\"] = 20\n",
    "plt.rcParams[\"legend.fontsize\"] = 20\n",
    "plt.rcParams[\"legend.markerscale\"] = 1.5\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "plt.rcParams[\"legend.title_fontsize\"] = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_GRAPHS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define the input and output folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"input/\"\n",
    "\n",
    "train_data_path = os.path.join(input_folder, \"train.csv\")\n",
    "test_data_path = os.path.join(input_folder, \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the traning data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the csv data to variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_data_path)\n",
    "\n",
    "test_data = pd.read_csv(test_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Data Investigation EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns that we will drop are:\n",
    "- PassengerId: it is just an index\n",
    "- Name: it is not relevant for the model\n",
    "- Ticket: it is not relevant for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "DROP_DATA = [\"PassengerId\", \"Name\", \"Ticket\"]\n",
    "def remove_Unused_Columns(data):\n",
    "    data = data.drop(DROP_DATA, axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "def convet_gender_to_numric(df):\n",
    "    \"\"\"   \n",
    "    \"sex\" column ===> male = 1, female = 0\n",
    "    \"\"\"\n",
    "    df['Sex'] = le.fit_transform(df['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = remove_Unused_Columns(train_data)\n",
    "\n",
    "# passenger_ids save the passenger ids for the test data for the submission\n",
    "passenger_ids = test_data['PassengerId']\n",
    "test_data = remove_Unused_Columns(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert Men/Women to 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convet_gender_to_numric(train_data)\n",
    "\n",
    "convet_gender_to_numric(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get summary statistics for the training dataset show only the numerical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of missing values in the training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains missing values in the following columns:\n",
    "\n",
    "1) Age: 177 missing values\n",
    "2) Cabin: 687 missing values (cabin has a lot of missing values)\n",
    "3) Embarked: 2 missing values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the data types of the columns in the training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that most of the data is int64 or float64, only the Cabin and Embarked Are object types (String)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_calculate(df, column):\n",
    "    # Plot the survival rate\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x=column, hue='Survived', data=df)\n",
    "    plt.title(f'Survival Rate by {column}')\n",
    "    plt.show()\n",
    "\n",
    "    # Group by column and 'Survived', then get the size of each group\n",
    "    grouped = df.groupby([column, 'Survived']).size()\n",
    "\n",
    "    # Calculate the percentage of survivors\n",
    "    percentage_survived = grouped.xs(1, level='Survived') / grouped.groupby(level=column).sum() * 100\n",
    "\n",
    "    # Print the percentage of survivors\n",
    "    print(f\"Percentage of survivors for each {column}:\")\n",
    "    print(percentage_survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we will explore the relationship between the survival rate and some of the features in the dataset.\n",
    "- After each graph, we will print the percentage of survivors for each category in the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_GRAPHS:\n",
    "    plot_and_calculate(train_data, 'Embarked')\n",
    "    plot_and_calculate(train_data, 'Parch')\n",
    "    plot_and_calculate(train_data, 'SibSp')\n",
    "    plot_and_calculate(train_data, 'Sex')\n",
    "    plot_and_calculate(train_data, 'Pclass')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can study from this graph\n",
    "1) From port C more people survived but from S and Q most of the people died\n",
    "2) Most women survived (74%)\n",
    "3) Most people from Pclass 1 survived (63%) but most people from Pclass 3 died (76% died)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pairplot Visualizing Correlation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we will display the pairplot of the data we can see the correlation between the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_GRAPHS:\n",
    "    sns.pairplot(train_data[['Survived', 'Pclass', 'Age', 'Fare', 'Sex']], hue='Survived')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heatmap for correlation matrix**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_GRAPHS:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    corr_matrix = train_data.corr(numeric_only=True)\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the correlation between the Survived and features like Pclass, Fare are high.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values in the train dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill Missing Values\n",
    "- Categorical columns with the most frequent value \n",
    "- Numerical columns with the mean\n",
    "- Drop the Cabin column due to too many missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# fill missing values, the \n",
    "\n",
    "\n",
    "def fill_missing_values(data):\n",
    "    data['Age'] = data['Age'].fillna(data['Age'].mean())\n",
    "    data['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])\n",
    "    data = handle_missing_values(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def drop_missing_values(data):\n",
    "    data.drop(columns=['Cabin'], inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def handle_missing_values(data):    \n",
    "    # Impute numerical columns with mean\n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    data[data.select_dtypes(include=['number']).columns] = imputer_num.fit_transform(data.select_dtypes(include=['number']))\n",
    "    \n",
    "    # Impute categorical columns with constant value 'missing'\n",
    "    imputer_cat = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "    data[data.select_dtypes(exclude=['number']).columns] = imputer_cat.fit_transform(data.select_dtypes(exclude=['number']))\n",
    "    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there's only two people with NaN, after searching about them it was find out they board in Southampton \n",
    "train_data[\"Embarked\"] = train_data.loc[:, 'Embarked'].fillna('S')\n",
    "\n",
    "train_data = drop_missing_values(train_data)\n",
    "train_data = fill_missing_values(train_data)\n",
    "\n",
    "# test data\n",
    "test_data = drop_missing_values(test_data)\n",
    "test_data = fill_missing_values(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.isnull().sum())\n",
    "\n",
    "print(test_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Categorical Variables into Numerical Values\n",
    "- Embarked use One-hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical(data):\n",
    "    # One-hot encode the categorical columns\n",
    "    data = pd.get_dummies(data, columns=['Embarked'], drop_first=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encode_categorical(train_data)\n",
    "\n",
    "test_data = encode_categorical(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize Numerical Features\n",
    "- Standardize Age and Fare to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_data(data):\n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "    data[['Age', 'Fare']] = scaler.fit_transform(data[['Age', 'Fare']])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = scale_data(train_data)\n",
    "\n",
    "test_data = scale_data(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Features\n",
    " - Family Size from SibSp and Parch\n",
    " - Is Alone from the FamilySize\n",
    " - Remove SibSp and Parch columns becuse of the new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(data):\n",
    "    \"\"\"\n",
    "    Create new features for the dataset.\n",
    "    \"\"\"\n",
    "    # Create FamilySize feature \n",
    "    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n",
    "\n",
    "    # Create IsAlone feature\n",
    "    data['IsAlone'] = (data['FamilySize'] == 1).astype(int)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def drop_features(data):\n",
    "    data.drop(columns=['SibSp', 'Parch'], inplace=True)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_data = create_features(train_data)\n",
    "train_data = drop_features(train_data)\n",
    "# test data\n",
    "test_data = create_features(test_data)\n",
    "test_data = drop_features(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.head())\n",
    "\n",
    "print(\"\\n\\nmissing values in train data:\\n\")\n",
    "print(train_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_GRAPHS:\n",
    "    corr_matrix = train_data.corr(numeric_only=True)\n",
    "    sorted_columns = corr_matrix.abs().sort_values('Survived', ascending=False).index\n",
    "    sorted_corr_matrix = corr_matrix.reindex(index=sorted_columns, columns=sorted_columns)\n",
    "\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=sorted_corr_matrix.values,\n",
    "        x=sorted_corr_matrix.columns,\n",
    "        y=sorted_corr_matrix.columns,\n",
    "        colorscale='Viridis',\n",
    "        text=sorted_corr_matrix.values.round(2),\n",
    "        texttemplate=\"%{text}\",\n",
    "        showscale=True\n",
    "    ))\n",
    "    fig.update_layout(title='Correlation Matrix Sorted by \"Survived\"', width=1000, height=800)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the data into features and target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(columns='Survived')\n",
    "t = train_data['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPI functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate key performance indicators (KPIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpis(cm):\n",
    "    \"\"\"\n",
    "    Calculate key performance indicators (KPIs) from a confusion matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    cm (numpy.ndarray): Confusion matrix\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing precision, recall, specificity, false positive rate, and accuracy.\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    fpr = fp / (fp + tn)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return {\n",
    "        'Precision': round(precision, 2),\n",
    "        'Recall': round(recall, 2),\n",
    "        'Specificity': round(specificity, 2),\n",
    "        'FPR': round(fpr, 2),\n",
    "        'Accuracy': round(accuracy, 2),\n",
    "        'F1 Score': round(f1, 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "def plot_kpi_table(kpi_values):\n",
    "    \"\"\"\n",
    "    Display KPIs in a table using Plotly.\n",
    "    \n",
    "    Parameters:\n",
    "    kpi_values (dict): Dictionary containing KPI names as keys and KPI values as values.\n",
    "    \"\"\"\n",
    "    fig = go.Figure(data=[go.Table(\n",
    "        header=dict(values=[\"KPI\", \"Value\"],\n",
    "                    fill_color='paleturquoise',\n",
    "                    align='left'),\n",
    "        cells=dict(values=[list(kpi_values.keys()), list(kpi_values.values())],\n",
    "                   fill_color='lavender',\n",
    "                   align='left'))\n",
    "    ])\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Key Performance Indicators (KPIs)\",\n",
    "        autosize=True,\n",
    "        width=500,\n",
    "        height=400\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Display the confusion matrix as a heatmap and print key performance indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_KPIs(cm):\n",
    "    \"\"\"\n",
    "    Display the confusion matrix as a heatmap using Plotly and print key performance indicators.\n",
    "    \n",
    "    Parameters:\n",
    "    cm (numpy.ndarray): Confusion matrix\n",
    "    \"\"\"\n",
    "    # Create a DataFrame for the confusion matrix with proper labels\n",
    "    cm_df = pd.DataFrame(cm, index=['actual_0', 'actual_1'], columns=['predicted_0', 'predicted_1'])\n",
    "    \n",
    "    # Plot the confusion matrix as a heatmap using Plotly\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=cm_df.values,\n",
    "        x=cm_df.columns,\n",
    "        y=cm_df.index,\n",
    "        colorscale='Blues',  \n",
    "        text=cm_df.values,\n",
    "        texttemplate=\"%{text}\",\n",
    "        hoverinfo=\"text\"\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Confusion Matrix\",\n",
    "        xaxis_title=\"Predicted Label\",\n",
    "        yaxis_title=\"Actual Label\",\n",
    "        font=dict(size=18)\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Calculate KPIs and show table\n",
    "    kpi_values = kpis(cm)\n",
    "    plot_kpi_table(kpi_values)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- calculate score and loss from cv (KFold or LPO) and display graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import RepeatedKFold, LeavePOut\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn import metrics\n",
    "\n",
    "def get_cv_score_and_loss(X, t, model, k=None, p=None, show_score_loss_graphs=True, show_cm=True):\n",
    "    # Initialize a DataFrame to store scores and losses\n",
    "    scores_losses_df = pd.DataFrame(columns=['fold_id', 'split', 'score', 'loss'])\n",
    "\n",
    "    # Determine the type of cross-validation to use\n",
    "    if k is not None:\n",
    "        cv = RepeatedKFold(n_splits=k, n_repeats=10, random_state=1)  # K-Fold cross-validation\n",
    "    elif p is not None:\n",
    "        cv = LeavePOut(p)  # Leave-P-Out cross-validation\n",
    "    else:\n",
    "        raise ValueError('You need to specify either k or p for cross-validation.')\n",
    "\n",
    "    # List to store confusion matrices for each fold\n",
    "    conf_matrix_list_of_arrays = []\n",
    "    \n",
    "    # Iterate over each fold in the cross-validation\n",
    "    for i, (train_ids, val_ids) in enumerate(cv.split(X)):\n",
    "        # Split the data into training and validation sets\n",
    "        X_train = X.loc[X.index.intersection(train_ids)]\n",
    "        t_train = t.loc[t.index.intersection(train_ids)]\n",
    "        X_val = X.loc[X.index.intersection(val_ids)]\n",
    "        t_val = t.loc[t.index.intersection(val_ids)]\n",
    "\n",
    "        # Fit the model on the training set\n",
    "        model.fit(X_train, t_train)\n",
    "\n",
    "        # Predict on the training and validation sets\n",
    "        y_train = abs(model.predict(X_train))\n",
    "        y_val = abs(model.predict(X_val))\n",
    "        \n",
    "        # Store scores and losses\n",
    "        scores_losses_df.loc[len(scores_losses_df)] = [i, 'train', model.score(X_train, t_train), metrics.log_loss(y_train, t_train)]\n",
    "        scores_losses_df.loc[len(scores_losses_df)] = [i, 'val', model.score(X_val, t_val), metrics.log_loss(y_val, t_val)]\n",
    "\n",
    "        # Compute the confusion matrix for the validation set and store it\n",
    "        conf_matrix = confusion_matrix(t_val, y_val)\n",
    "        conf_matrix_list_of_arrays.append(conf_matrix)\n",
    "\n",
    "    # Separate the scores and losses for training and validation sets\n",
    "    val_scores_losses_df = scores_losses_df[scores_losses_df['split'] == 'val']\n",
    "    train_scores_losses_df = scores_losses_df[scores_losses_df['split'] == 'train']\n",
    "\n",
    "    # Calculate mean scores and losses\n",
    "    mean_val_score = val_scores_losses_df['score'].mean()\n",
    "    mean_val_loss = val_scores_losses_df['loss'].mean()\n",
    "    mean_train_score = train_scores_losses_df['score'].mean()\n",
    "    mean_train_loss = train_scores_losses_df['loss'].mean()\n",
    "\n",
    "    # Calculate the mean confusion matrix across all folds\n",
    "    mean_of_conf_matrix_arrays = np.mean(conf_matrix_list_of_arrays, axis=0)\n",
    "\n",
    "    # Plot the score and loss graphs if requested\n",
    "    if show_score_loss_graphs:\n",
    "        fig = px.line(scores_losses_df, x='fold_id', y='score', color='split', title=f'Mean Val Score: {mean_val_score:.2f}, Mean Train Score: {mean_train_score:.2f}')\n",
    "        fig.show()\n",
    "        fig = px.line(scores_losses_df, x='fold_id', y='loss', color='split', title=f'Mean Val Loss (CE): {mean_val_loss:.2f}, Mean Train Loss (CE): {mean_train_loss:.2f}')\n",
    "        fig.show()\n",
    "\n",
    "    # Display the confusion matrix KPIs if requested\n",
    "    if show_cm:\n",
    "        confusion_matrix_KPIs(mean_of_conf_matrix_arrays)\n",
    "        \n",
    "\n",
    "    return mean_val_score, mean_val_loss, mean_train_score, mean_train_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we will search for the strongest features that affect the survival rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def feature_selec(X, y, n):\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'bool']).columns\n",
    "    all_cols = categorical_cols.tolist() + numerical_cols.tolist()\n",
    "    ct_enc_std = ColumnTransformer([\n",
    "                (\"encoding\", OrdinalEncoder(), categorical_cols),\n",
    "                (\"standard\", StandardScaler(), numerical_cols)])\n",
    "    X_encoded = pd.DataFrame(ct_enc_std.fit_transform(X, y), columns=all_cols)\n",
    "\n",
    "    selector = RFE(SGDRegressor(random_state=42), n_features_to_select=n).\\\n",
    "    fit(X_encoded, y)\n",
    "\n",
    "    X_encoded.loc[:, selector.support_]\n",
    "\n",
    "    # print the fetures selection list\n",
    "    features = X_encoded.loc[:, selector.support_].columns.tolist()\n",
    "    print(\"features: \", features)\n",
    "\n",
    "    # keep only the feature selection list\n",
    "    X = X[features]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def find_best_feature_count(X, y, model=None, cv_splits=5, cv_repeats=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Function to find the optimal number of features using RFECV and plot the cross-validation score.\n",
    "    \n",
    "    Parameters:\n",
    "    X (pd.DataFrame): Feature set.\n",
    "    y (pd.Series): Target variable.\n",
    "    model (sklearn estimator): The model to use for feature selection. Defaults to SGDClassifier.\n",
    "    cv_splits (int): Number of splits for cross-validation.\n",
    "    cv_repeats (int): Number of repeats for cross-validation.\n",
    "    random_state (int): Random state for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    int: Optimal number of features to use.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model = SGDClassifier(loss='log_loss', random_state=random_state)\n",
    "    \n",
    "    max_features = X.shape[1]  # Maximum number of features to select\n",
    "    \n",
    "    # Initialize RFECV with the model and cross-validation strategy\n",
    "    selector = RFECV(estimator=model, cv=RepeatedKFold(n_splits=cv_splits, n_repeats=cv_repeats, random_state=random_state), \n",
    "                     min_features_to_select=1, step=1)\n",
    "    \n",
    "    # Fit RFECV\n",
    "    selector.fit(X, y)\n",
    "    \n",
    "    # Get the optimal number of features\n",
    "    optimal_feature_count = selector.n_features_\n",
    "    \n",
    "    # Plot the cross-validation scores\n",
    "    fig = go.Figure()\n",
    "    results = selector.cv_results_['mean_test_score']  # Getting the mean cv score for each set of features\n",
    "    fig.add_trace(go.Scatter(x=[i for i in range(1, len(results) + 1)], y=results))\n",
    "    fig.update_xaxes(title_text=\"Number of features selected\")\n",
    "    fig.update_yaxes(title_text=\"Cross-validation score (number of correct classifications)\")\n",
    "    fig.show()\n",
    "    \n",
    "    return optimal_feature_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = find_best_feature_count(X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best N features \n",
    "num_of_fetures = 4\n",
    "\n",
    "\n",
    "# make sure the number of features is less than the number of columns\n",
    "if num_of_fetures > train_data.shape[1] -1:\n",
    "    num_of_fetures = train_data.shape[1] -1\n",
    "\n",
    "\n",
    "\n",
    "best_features = feature_selec(X, t, num_of_fetures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from scipy.stats import uniform\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compare_search_methods(X, t, cv_splits=5, cv_repeats=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Function to compare GridSearchCV and RandomizedSearchCV for parameter optimization of SGDClassifier.\n",
    "    Performs feature selection using RFECV and plots the comparison graph.\n",
    "    \n",
    "    Parameters:\n",
    "    X (pd.DataFrame): Feature set.\n",
    "    t (pd.Series): Target variable.\n",
    "    cv_splits (int): Number of splits for cross-validation.\n",
    "    cv_repeats (int): Number of repeats for cross-validation.\n",
    "    random_state (int): Random state for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing the best hyperparameters and scores for GridSearchCV and RandomizedSearchCV.\n",
    "    \"\"\"\n",
    "    # Feature selection using RFECV\n",
    "    selector = RFECV(estimator=SGDClassifier(loss='log_loss', random_state=random_state),\n",
    "                     cv=RepeatedKFold(n_splits=cv_splits, n_repeats=cv_repeats, random_state=random_state))\n",
    "    X_selected_features = pd.DataFrame(selector.fit_transform(X, t), columns=X.columns[selector.support_])\n",
    "    \n",
    "    # GridSearchCV\n",
    "    hyper_parameters_grid = {'penalty': ('l2', 'l1', 'elasticnet'), 'alpha': [0.0001, 0.001, 0.01, 0.1]}\n",
    "    gs_model = GridSearchCV(SGDClassifier(random_state=random_state), hyper_parameters_grid, cv=5)\n",
    "    gs_model.fit(X_selected_features, t)\n",
    "    \n",
    "    # RandomizedSearchCV\n",
    "    distributions = {'alpha': uniform(loc=0, scale=1), 'penalty': ['l2', 'l1', 'elasticnet']}\n",
    "    rs_model = RandomizedSearchCV(SGDClassifier(random_state=random_state), distributions, cv=5, random_state=random_state)\n",
    "    rs_model.fit(X_selected_features, t)\n",
    "    \n",
    "    # Print results\n",
    "    print('GridSearchCV results:')\n",
    "    print('Accuracy score:', gs_model.best_score_)\n",
    "    print('Best parameters:', gs_model.best_params_)\n",
    "    print()\n",
    "    \n",
    "    print('RandomizedSearchCV results:')\n",
    "    print('Accuracy score:', rs_model.best_score_)\n",
    "    print('Best parameters:', rs_model.best_params_)\n",
    "    print()\n",
    "    \n",
    "    # Plotting the comparison graph\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(name='GridSearchCV', x=['Accuracy'], y=[gs_model.best_score_], marker_color='blue'))\n",
    "    fig.add_trace(go.Bar(name='RandomizedSearchCV', x=['Accuracy'], y=[rs_model.best_score_], marker_color='orange'))\n",
    "    fig.update_layout(title='Comparison of GridSearchCV and RandomizedSearchCV',\n",
    "                      xaxis_title='Search Method',\n",
    "                      yaxis_title='Accuracy Score',\n",
    "                      barmode='group')\n",
    "    fig.show()\n",
    "    \n",
    "    # Return the best hyperparameters and scores\n",
    "    best_params = {\n",
    "        'GridSearchCV': {\n",
    "            'best_score': gs_model.best_score_,\n",
    "            'best_params': gs_model.best_params_\n",
    "        },\n",
    "        'RandomizedSearchCV': {\n",
    "            'best_score': rs_model.best_score_,\n",
    "            'best_params': rs_model.best_params_\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return gs_model, rs_model, best_params\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Search VS Random Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model, rs_model, best_params = compare_search_methods(X, t)\n",
    "print(best_params )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = RFECV(estimator=SGDClassifier(loss='log_loss', random_state=42),\n",
    "                 cv=RepeatedKFold(n_splits=5, n_repeats=10, random_state=42))\n",
    "selector.fit(X, t)\n",
    "selected_features = X.columns[selector.support_]\n",
    "best_features = X[selected_features]\n",
    "\n",
    "# Calculate scores and losses\n",
    "val_score, val_loss, train_score, train_loss = get_cv_score_and_loss(best_features, t, SGDClassifier(loss='log_loss', random_state=42), k=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can see that the difference is not significant, and the grid search is more expensive than the random search.<br> Despite this, we will stick with the grid search for now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titanic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
